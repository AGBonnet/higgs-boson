{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL: CS-433 Machine Learning\n",
    "### Project 1: Higgs Boson\n",
    "\n",
    "Team: GoLinearOrGoHome (Antoine Bonnet, Melanie X, Camille X)\n",
    "\n",
    "In this project, we train multiple models for the task of binary classification on the Higgs Boson dataset. \n",
    "We compare their respective performances and choose the model with highest accuracy. This optimal model is then used to predict unseen test data. \n",
    "\n",
    "\n",
    "### 1. Loading data\n",
    "\n",
    "We first import required libraries and our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from implementations import *\n",
    "from load_data import *\n",
    "from train import *\n",
    "from helpers import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these experiments, we perform 5-fold cross-validation hyperparameter tuning with grid search for every model using 200K of our 250K training set. We use the remaining 50K samples as a synthetic test set to measure the generalization capacity of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30) (200000,)\n",
      "(50000, 30) (50000,)\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels, training_ids, testing_data, testing_ids = load_data(sub_sample=False)\n",
    "train_data = training_data[:200000]\n",
    "train_labels = training_labels[:200000]\n",
    "test_data = training_data[200000:]\n",
    "test_labels = training_labels[200000:]\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,Prediction,DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt\n",
      "100000,s,138.47,51.655,97.827,27.98,0.91,124.711,2.666,3.064,41.928,197.76,1.582,1.396,0.2,32.638,1.017,0.381,51.626,2.273,-2.414,16.824,-0.277,258.733,2,67.435,2.15,0.444,46.062,1.24,-2.475,113.497\n",
      "100001,b,160.937,68.768,103.235,48.146,-999,-999,-999,3.473,2.078,125.157,0.879,1.414,-999,42.014,2.039,-3.011,36.918,0.501,0.103,44.704,-1.916,164.546,1,46.226,0.725,1.158,-999,-999,-999,46.226\n",
      "100002,b,-999,162.172,125.953,35.635,-999,-999,-999,3.148,9.336,197.814,3.776,1.414,-999,32.154,-0.705,-2.093,121.409,-0.953,1.052,54.283,-2.186,260.414,1,44.251,2.053,-2.028,-999,-999,-999,44.251\n",
      "100003,b,143.905,81.417,80.943,0.414,-999,-999,-999,3.31,0.414,75.968,2.354,-1.285,-999,22.647,-1.655,0.01,53.321,-0.522,-3.1,31.082,0.06,86.062,0,-999,-999,-999,-999,-999,-999,0\n",
      "100004,b,175.864,16.915,134.805,16.405,-999,-999,-999,3.891,16.405,57.983,1.056,-1.385,-999,28.209,-2.197,-2.231,29.774,0.798,1.569,2.723,-0.871,53.131,0,-999,-999,-999,-999,-999,-999,0\n",
      "100005,b,89.744,13.55,59.149,116.344,2.636,284.584,-0.54,1.362,61.619,278.876,0.588,0.479,0.975,53.651,0.371,1.329,31.565,-0.884,1.857,40.735,2.237,282.849,3,90.547,-2.412,-0.653,56.165,0.224,3.106,193.66\n"
     ]
    }
   ],
   "source": [
    "!head -7 ../data/train.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,Prediction,DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt\n",
      "350000,?,-999,79.589,23.916,3.036,-999,-999,-999,0.903,3.036,56.018,1.536,-1.404,-999,22.088,-0.54,-0.609,33.93,-0.504,-1.511,48.509,2.022,98.556,0,-999,-999,-999,-999,-999,-999,0\n",
      "350001,?,106.398,67.49,87.949,49.994,-999,-999,-999,2.048,2.679,132.865,1.777,-1.204,-999,30.716,-1.784,3.054,54.574,-0.169,1.795,21.093,-1.138,176.251,1,47.575,-0.553,-0.849,-999,-999,-999,47.575\n",
      "350002,?,117.794,56.226,96.358,4.137,-999,-999,-999,2.755,4.137,97.6,1.096,-1.408,-999,46.564,-0.298,3.079,51.036,-0.548,0.336,19.461,-1.868,111.505,0,-999,-999,-999,-999,-999,-999,0\n",
      "350003,?,135.861,30.604,97.288,9.104,-999,-999,-999,2.811,9.104,94.112,0.819,-1.382,-999,51.741,0.388,-1.408,42.371,-0.295,2.148,25.131,1.172,164.707,0,-999,-999,-999,-999,-999,-999,0\n",
      "350004,?,74.159,82.772,58.731,89.646,1.347,536.663,-0.339,1.028,77.213,721.552,1.713,-0.913,0.004,45.087,-1.548,1.877,77.252,-1.913,2.838,22.2,-0.231,869.614,3,254.085,-1.013,-0.334,185.857,0.335,2.587,599.213\n",
      "350005,?,95.709,94.168,66.28,14.719,-999,-999,-999,3.065,14.719,70.259,2.465,-1.351,-999,20.276,-1.212,2.289,49.983,-0.63,-0.72,44.585,2.566,86.129,0,-999,-999,-999,-999,-999,-999,0\n"
     ]
    }
   ],
   "source": [
    "!head -7 ../data/test.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training labels consist of either 's' or 'b' values, which respectively denote 'signal' (presence of the Higgs Boson) and 'background' (absence of the Higgs Boson). The test set contains only features and no labels. Our goal is to fill in these labels with highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values are missing from the data, as indicated by the value -999. We look at the proportion of missing values for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15263 , 0.      , 0.      , 0.      , 0.710245, 0.710245,\n",
       "       0.710245, 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.710245, 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.39983 ,\n",
       "       0.39983 , 0.39983 , 0.710245, 0.710245, 0.710245, 0.      ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data == -999).sum(axis=0) / train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15176, 0.     , 0.     , 0.     , 0.70816, 0.70816, 0.70816,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.70816, 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.39894, 0.39894, 0.39894, 0.70816, 0.70816,\n",
       "       0.70816, 0.     ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_data == -999).sum(axis=0) / test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing data\n",
    "\n",
    "To train our models, we extract the training features and label, as well as the test features. Our pre-processing consists of the following procedures: \n",
    "- **Replacing missing values**: columns containing more than $\\alpha$% of missing values (indicated by -999) are removed, and the remaining missing values are replaced by their respective median.\n",
    "- **Removing outliers**: datapoints which contain at least one feature that is $\\beta$ times standard deviations away from the mean are removed. This is only performed for the training data (and not the test data).\n",
    "- **Polynomial expansion**: Each feature in the data matrix is augmented by a given degree. A column of 1's is also added for biasing. \n",
    "- **Standardization**: All columns are standardized to follow a standard normal distribution $N(0,1)$.\n",
    "\n",
    "These operations are carried out in the `pre_process` function. We will perform hyperparameter tuning on the degree as well as the $\\alpha$ and $\\beta$ thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Models \n",
    "\n",
    "We now train different models for this binary classification task. For each model, we consider different combinations of the pre-processing hyperparameter values, as well as the respective model hyperparameter values. We use 5-fold cross validation to select the hyperparameter set with the highest validation accuracy. We then obtain the optimal validation accuracy for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # Number of folds for cross-validation\n",
    "verbose = True # Choose if you want to print the accuracies for each hyperparameter set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy(model_name, optPreHyp, optHyp):\n",
    "    # Pre-process whole training set with optimal pre-processing hyperparameters\n",
    "    x_train_opt, y_train_opt = pre_process(x=train_data, y=train_labels, **optPreHyp)\n",
    "\n",
    "    # Train model with optimal hyperparameters and whole training set\n",
    "    wOpt, _ = model_name(y=y_train_opt, tx=x_train_opt, **optHyp)\n",
    "\n",
    "    # Pre-process whole test set but keep outlier rows\n",
    "    fullOptPreHyp = optPreHyp\n",
    "    fullOptPreHyp['beta'] = None\n",
    "    x_opt_test, _ = pre_process(x=test_data, y=test_labels, **fullOptPreHyp)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    testPred = prediction(wOpt, x_opt_test)\n",
    "    testAccuracy = evaluate_accuracy(testPred, test_labels)\n",
    "    print('Test accuracy: ', testAccuracy) \n",
    "    return testAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Least squares regression\n",
    "\n",
    "We now train the least squares model, which computes weights $w^* = (X^T X)^{-1} X^T y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 30), (200000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.7446, 0.7444, 0.7452, 0.7447, 0.7441]  average = 0.7446\n",
      "\tValidation accuracies: [0.7433, 0.7453, 0.7435, 0.7426, 0.7466]  average = 0.7443\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.7713, 0.7717, 0.7716, 0.7721, 0.7703]  average = 0.7714\n",
      "\tValidation accuracies: [0.7722, 0.7696, 0.7687, 0.771, 0.774]  average = 0.7711\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.7186, 0.6873, 0.7259, 0.714, 0.5066]  average = 0.6705\n",
      "\tValidation accuracies: [0.7152, 0.6888, 0.726, 0.7101, 0.508]  average = 0.6696\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.5909, 0.6947, 0.6703, 0.6483, 0.5561]  average = 0.6321\n",
      "\tValidation accuracies: [0.5911, 0.6918, 0.668, 0.6478, 0.5602]  average = 0.6318\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.6352, 0.5676, 0.6681, 0.4779, 0.6686]  average = 0.6035\n",
      "\tValidation accuracies: [0.6346, 0.5638, 0.6674, 0.4782, 0.6687]  average = 0.6025\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': None, 'beta': None}\n",
      "\tTraining accuracies:   [0.5404, 0.5464, 0.4502, 0.4885, 0.531]  average = 0.5113\n",
      "\tValidation accuracies: [0.5431, 0.5406, 0.4484, 0.492, 0.5325]  average = 0.5113\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7508, 0.7504, 0.7503, 0.7505, 0.7504]  average = 0.7505\n",
      "\tValidation accuracies: [0.7493, 0.7514, 0.7501, 0.7492, 0.752]  average = 0.7504\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7811, 0.7811, 0.7801, 0.7808, 0.78]  average = 0.7806\n",
      "\tValidation accuracies: [0.78, 0.7771, 0.781, 0.7803, 0.7834]  average = 0.7804\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.5616, 0.728, 0.7116, 0.7385, 0.7278]  average = 0.6935\n",
      "\tValidation accuracies: [0.5648, 0.7271, 0.7069, 0.7392, 0.7238]  average = 0.6924\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.6248, 0.4653, 0.7323, 0.5428, 0.5886]  average = 0.5908\n",
      "\tValidation accuracies: [0.6229, 0.4662, 0.7331, 0.5449, 0.5897]  average = 0.5914\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.6732, 0.4264, 0.5721, 0.6309, 0.6198]  average = 0.5845\n",
      "\tValidation accuracies: [0.6708, 0.4264, 0.5674, 0.6345, 0.6185]  average = 0.5835\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': None, 'beta': 5}\n",
      "\tTraining accuracies:   [0.4795, 0.4972, 0.5638, 0.5365, 0.4535]  average = 0.5061\n",
      "\tValidation accuracies: [0.4792, 0.4992, 0.5675, 0.5368, 0.451]  average = 0.5067\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7472, 0.7465, 0.7475, 0.7458, 0.7462]  average = 0.7466\n",
      "\tValidation accuracies: [0.7456, 0.7464, 0.7447, 0.7492, 0.7465]  average = 0.7465\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7755, 0.7757, 0.7749, 0.7757, 0.7747]  average = 0.7753\n",
      "\tValidation accuracies: [0.7744, 0.7749, 0.7753, 0.7745, 0.7772]  average = 0.7753\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.713, 0.7125, 0.6899, 0.7279, 0.7242]  average = 0.7135\n",
      "\tValidation accuracies: [0.7128, 0.7126, 0.688, 0.7304, 0.7201]  average = 0.7128\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.5117, 0.707, 0.7127, 0.5886, 0.6163]  average = 0.6273\n",
      "\tValidation accuracies: [0.5138, 0.7089, 0.7123, 0.5845, 0.6175]  average = 0.6274\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.631, 0.5912, 0.5798, 0.7129, 0.4289]  average = 0.5888\n",
      "\tValidation accuracies: [0.6274, 0.5931, 0.5824, 0.7109, 0.4315]  average = 0.5891\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': None, 'beta': 8}\n",
      "\tTraining accuracies:   [0.5065, 0.5421, 0.4585, 0.4872, 0.5955]  average = 0.518\n",
      "\tValidation accuracies: [0.5125, 0.5487, 0.4552, 0.4844, 0.5975]  average = 0.5197\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7456, 0.746, 0.7448, 0.7457, 0.7459]  average = 0.7456\n",
      "\tValidation accuracies: [0.7446, 0.7446, 0.7489, 0.7457, 0.7448]  average = 0.7457\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7747, 0.774, 0.7734, 0.7733, 0.7744]  average = 0.774\n",
      "\tValidation accuracies: [0.7718, 0.7739, 0.7747, 0.7761, 0.771]  average = 0.7735\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.6527, 0.4855, 0.6986, 0.7445, 0.7279]  average = 0.6618\n",
      "\tValidation accuracies: [0.6492, 0.4869, 0.7003, 0.7417, 0.7269]  average = 0.661\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.663, 0.6234, 0.4389, 0.6897, 0.699]  average = 0.6228\n",
      "\tValidation accuracies: [0.663, 0.6271, 0.4437, 0.6902, 0.6946]  average = 0.6237\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.6894, 0.7, 0.6926, 0.4805, 0.5139]  average = 0.6153\n",
      "\tValidation accuracies: [0.6853, 0.7009, 0.6919, 0.4811, 0.5153]  average = 0.6149\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': None, 'beta': 10}\n",
      "\tTraining accuracies:   [0.4204, 0.5064, 0.4758, 0.4537, 0.5899]  average = 0.4892\n",
      "\tValidation accuracies: [0.42, 0.5036, 0.475, 0.4538, 0.5921]  average = 0.4889\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7453, 0.7455, 0.7456, 0.744, 0.7453]  average = 0.7451\n",
      "\tValidation accuracies: [0.7456, 0.7433, 0.7433, 0.7495, 0.7434]  average = 0.745\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7726, 0.773, 0.7728, 0.773, 0.7736]  average = 0.773\n",
      "\tValidation accuracies: [0.7743, 0.7745, 0.7726, 0.7728, 0.77]  average = 0.7728\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.5132, 0.5809, 0.7306, 0.6874, 0.6644]  average = 0.6353\n",
      "\tValidation accuracies: [0.5113, 0.5863, 0.7294, 0.6879, 0.6609]  average = 0.6352\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.6744, 0.6161, 0.6687, 0.6649, 0.6927]  average = 0.6634\n",
      "\tValidation accuracies: [0.6779, 0.6144, 0.6718, 0.6645, 0.6932]  average = 0.6644\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.6713, 0.664, 0.5194, 0.6723, 0.4981]  average = 0.605\n",
      "\tValidation accuracies: [0.6723, 0.663, 0.5181, 0.6697, 0.4961]  average = 0.6038\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': None, 'beta': 12}\n",
      "\tTraining accuracies:   [0.5251, 0.5768, 0.4835, 0.537, 0.559]  average = 0.5363\n",
      "\tValidation accuracies: [0.5252, 0.5831, 0.4853, 0.5393, 0.5587]  average = 0.5383\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.7344, 0.733, 0.7334, 0.7336, 0.7339]  average = 0.7337\n",
      "\tValidation accuracies: [0.7319, 0.7349, 0.7348, 0.7317, 0.7343]  average = 0.7335\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.7663, 0.7648, 0.7667, 0.7652, 0.766]  average = 0.7658\n",
      "\tValidation accuracies: [0.7666, 0.7674, 0.7623, 0.7665, 0.7656]  average = 0.7657\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.7793, 0.7782, 0.779, 0.778, 0.779]  average = 0.7787\n",
      "\tValidation accuracies: [0.7786, 0.779, 0.7758, 0.7822, 0.7767]  average = 0.7785\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.7868, 0.787, 0.7863, 0.7859, 0.7863]  average = 0.7865\n",
      "\tValidation accuracies: [0.7872, 0.7905, 0.786, 0.7849, 0.7826]  average = 0.7862\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.789, 0.7884, 0.7896, 0.7882, 0.557]  average = 0.7424\n",
      "\tValidation accuracies: [0.7876, 0.7928, 0.7899, 0.7881, 0.5549]  average = 0.7427\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 0.5, 'beta': None}\n",
      "\tTraining accuracies:   [0.7911, 0.7906, 0.7858, 0.7762, 0.736]  average = 0.7759\n",
      "\tValidation accuracies: [0.7912, 0.7902, 0.7836, 0.7762, 0.7377]  average = 0.7758\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7424, 0.7434, 0.7422, 0.7428, 0.7438]  average = 0.7429\n",
      "\tValidation accuracies: [0.7454, 0.7404, 0.7455, 0.7432, 0.7399]  average = 0.7429\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.781, 0.7824, 0.7808, 0.7806, 0.7818]  average = 0.7813\n",
      "\tValidation accuracies: [0.7823, 0.7767, 0.783, 0.7842, 0.7791]  average = 0.7811\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7922, 0.7909, 0.7918, 0.7904, 0.7917]  average = 0.7914\n",
      "\tValidation accuracies: [0.7888, 0.7927, 0.7903, 0.7943, 0.7899]  average = 0.7912\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.576, 0.5452, 0.7487, 0.79, 0.7949]  average = 0.691\n",
      "\tValidation accuracies: [0.5799, 0.5482, 0.7485, 0.7875, 0.7949]  average = 0.6918\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7648, 0.7061, 0.6848, 0.6055, 0.7092]  average = 0.6941\n",
      "\tValidation accuracies: [0.7617, 0.7055, 0.6904, 0.6055, 0.7067]  average = 0.694\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 0.5, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7696, 0.767, 0.7978, 0.8027, 0.7956]  average = 0.7865\n",
      "\tValidation accuracies: [0.7717, 0.7682, 0.794, 0.8021, 0.7975]  average = 0.7867\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7371, 0.7371, 0.7371, 0.7373, 0.7379]  average = 0.7373\n",
      "\tValidation accuracies: [0.7382, 0.7372, 0.7389, 0.7366, 0.7354]  average = 0.7373\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7738, 0.7732, 0.7729, 0.7741, 0.7735]  average = 0.7735\n",
      "\tValidation accuracies: [0.7725, 0.7753, 0.7753, 0.7711, 0.7723]  average = 0.7733\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7882, 0.7881, 0.7881, 0.7891, 0.7877]  average = 0.7882\n",
      "\tValidation accuracies: [0.7884, 0.7891, 0.7893, 0.784, 0.7902]  average = 0.7882\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7917, 0.7923, 0.7925, 0.7922, 0.792]  average = 0.7921\n",
      "\tValidation accuracies: [0.7939, 0.7898, 0.7899, 0.7935, 0.793]  average = 0.792\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7405, 0.7568, 0.7601, 0.7979, 0.7996]  average = 0.771\n",
      "\tValidation accuracies: [0.7386, 0.7588, 0.7645, 0.7966, 0.7943]  average = 0.7706\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 0.5, 'beta': 8}\n",
      "\tTraining accuracies:   [0.5923, 0.8029, 0.6112, 0.8028, 0.8094]  average = 0.7237\n",
      "\tValidation accuracies: [0.5912, 0.8068, 0.611, 0.8038, 0.8088]  average = 0.7243\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7358, 0.7349, 0.736, 0.7356, 0.736]  average = 0.7357\n",
      "\tValidation accuracies: [0.7348, 0.7379, 0.7351, 0.7358, 0.7333]  average = 0.7354\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.771, 0.7712, 0.7697, 0.771, 0.7705]  average = 0.7707\n",
      "\tValidation accuracies: [0.7693, 0.7688, 0.7746, 0.7691, 0.7705]  average = 0.7705\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7863, 0.7862, 0.7858, 0.7862, 0.7859]  average = 0.7861\n",
      "\tValidation accuracies: [0.785, 0.7837, 0.7871, 0.7859, 0.7877]  average = 0.7859\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7786, 0.6555, 0.7742, 0.7897, 0.7917]  average = 0.7579\n",
      "\tValidation accuracies: [0.7763, 0.6521, 0.7754, 0.7907, 0.7894]  average = 0.7568\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7833, 0.5421, 0.7523, 0.7825, 0.7931]  average = 0.7307\n",
      "\tValidation accuracies: [0.7818, 0.5475, 0.7565, 0.7767, 0.794]  average = 0.7313\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 0.5, 'beta': 10}\n",
      "\tTraining accuracies:   [0.8013, 0.7901, 0.7339, 0.7983, 0.8058]  average = 0.7859\n",
      "\tValidation accuracies: [0.8049, 0.7884, 0.7328, 0.7931, 0.8048]  average = 0.7848\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7353, 0.7345, 0.7345, 0.7348, 0.7353]  average = 0.7349\n",
      "\tValidation accuracies: [0.7317, 0.7358, 0.7357, 0.7353, 0.7355]  average = 0.7348\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7689, 0.7688, 0.7689, 0.7696, 0.7692]  average = 0.7691\n",
      "\tValidation accuracies: [0.7698, 0.77, 0.7707, 0.7657, 0.769]  average = 0.769\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.784, 0.7844, 0.7836, 0.7838, 0.7848]  average = 0.7841\n",
      "\tValidation accuracies: [0.786, 0.7826, 0.7856, 0.7846, 0.7798]  average = 0.7837\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7903, 0.7895, 0.7908, 0.7893, 0.7901]  average = 0.79\n",
      "\tValidation accuracies: [0.7915, 0.7924, 0.7876, 0.7879, 0.7891]  average = 0.7897\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7915, 0.7936, 0.788, 0.7927, 0.4741]  average = 0.728\n",
      "\tValidation accuracies: [0.7926, 0.7921, 0.7856, 0.7918, 0.4704]  average = 0.7265\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 0.5, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7999, 0.7981, 0.7996, 0.7767, 0.7885]  average = 0.7926\n",
      "\tValidation accuracies: [0.7977, 0.8004, 0.804, 0.7746, 0.7855]  average = 0.7924\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.7452, 0.7456, 0.7448, 0.7437, 0.7452]  average = 0.7449\n",
      "\tValidation accuracies: [0.7458, 0.743, 0.7451, 0.7475, 0.7431]  average = 0.7449\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.775, 0.7752, 0.774, 0.7752, 0.7752]  average = 0.7749\n",
      "\tValidation accuracies: [0.7736, 0.7771, 0.7788, 0.7721, 0.7732]  average = 0.775\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.7854, 0.7848, 0.7855, 0.7859, 0.7858]  average = 0.7855\n",
      "\tValidation accuracies: [0.785, 0.7867, 0.7866, 0.7848, 0.783]  average = 0.7852\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.793, 0.7933, 0.7882, 0.7927, 0.7921]  average = 0.7919\n",
      "\tValidation accuracies: [0.7911, 0.7922, 0.7878, 0.791, 0.7952]  average = 0.7915\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.7966, 0.7964, 0.7727, 0.7766, 0.7772]  average = 0.7839\n",
      "\tValidation accuracies: [0.7983, 0.7957, 0.7736, 0.7762, 0.7777]  average = 0.7843\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': None}\n",
      "\tTraining accuracies:   [0.7964, 0.7964, 0.7935, 0.772, 0.7968]  average = 0.791\n",
      "\tValidation accuracies: [0.7941, 0.7973, 0.7909, 0.7724, 0.7978]  average = 0.7905\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.752, 0.753, 0.7523, 0.7528, 0.7523]  average = 0.7525\n",
      "\tValidation accuracies: [0.7551, 0.7495, 0.7542, 0.7499, 0.7537]  average = 0.7525\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.788, 0.7872, 0.7871, 0.7879, 0.788]  average = 0.7876\n",
      "\tValidation accuracies: [0.7855, 0.7899, 0.7899, 0.787, 0.7847]  average = 0.7874\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.7961, 0.796, 0.796, 0.7968, 0.796]  average = 0.7962\n",
      "\tValidation accuracies: [0.796, 0.7961, 0.796, 0.7936, 0.7969]  average = 0.7957\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.8032, 0.8028, 0.8035, 0.8015, 0.7669]  average = 0.7956\n",
      "\tValidation accuracies: [0.8051, 0.8024, 0.8027, 0.7983, 0.7699]  average = 0.7957\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.6306, 0.4811, 0.6664, 0.7074, 0.8046]  average = 0.658\n",
      "\tValidation accuracies: [0.6298, 0.4835, 0.664, 0.7081, 0.8083]  average = 0.6587\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 5}\n",
      "\tTraining accuracies:   [0.6747, 0.6271, 0.5939, 0.7709, 0.7305]  average = 0.6794\n",
      "\tValidation accuracies: [0.6745, 0.6273, 0.5922, 0.7698, 0.7308]  average = 0.6789\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7492, 0.7475, 0.7486, 0.7479, 0.7475]  average = 0.7481\n",
      "\tValidation accuracies: [0.7451, 0.7506, 0.7455, 0.7474, 0.751]  average = 0.7479\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7807, 0.7819, 0.7823, 0.7815, 0.782]  average = 0.7817\n",
      "\tValidation accuracies: [0.7855, 0.7811, 0.7771, 0.7828, 0.7804]  average = 0.7814\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7946, 0.794, 0.7935, 0.7939, 0.7938]  average = 0.794\n",
      "\tValidation accuracies: [0.7918, 0.7943, 0.7951, 0.792, 0.7949]  average = 0.7936\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.7971, 0.7987, 0.7979, 0.7967, 0.7985]  average = 0.7978\n",
      "\tValidation accuracies: [0.7992, 0.7967, 0.7979, 0.8004, 0.792]  average = 0.7972\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.8053, 0.8004, 0.8052, 0.8023, 0.8033]  average = 0.8033\n",
      "\tValidation accuracies: [0.808, 0.7987, 0.8044, 0.8007, 0.8017]  average = 0.8027\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 8}\n",
      "\tTraining accuracies:   [0.8152, 0.8117, 0.8078, 0.609, 0.7952]  average = 0.7678\n",
      "\tValidation accuracies: [0.8162, 0.8102, 0.8046, 0.605, 0.7997]  average = 0.7671\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7475, 0.7466, 0.7464, 0.7463, 0.7466]  average = 0.7467\n",
      "\tValidation accuracies: [0.7446, 0.7455, 0.7468, 0.7486, 0.7471]  average = 0.7465\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7798, 0.7791, 0.7778, 0.779, 0.7794]  average = 0.779\n",
      "\tValidation accuracies: [0.7759, 0.7791, 0.7833, 0.7786, 0.7774]  average = 0.7789\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7917, 0.7923, 0.7922, 0.7917, 0.792]  average = 0.792\n",
      "\tValidation accuracies: [0.7916, 0.7909, 0.7914, 0.7926, 0.7907]  average = 0.7914\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.7966, 0.7977, 0.797, 0.7976, 0.7974]  average = 0.7973\n",
      "\tValidation accuracies: [0.7984, 0.7958, 0.7977, 0.7944, 0.7975]  average = 0.7968\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.8003, 0.802, 0.8017, 0.6176, 0.8014]  average = 0.7646\n",
      "\tValidation accuracies: [0.8008, 0.7991, 0.7981, 0.6163, 0.8026]  average = 0.7634\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "\tTraining accuracies:   [0.8091, 0.8109, 0.8034, 0.8059, 0.8109]  average = 0.808\n",
      "\tValidation accuracies: [0.8092, 0.8093, 0.8033, 0.8052, 0.8098]  average = 0.8074\n",
      "Pre-processing hyperparameters: {'degree': 1, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7457, 0.7455, 0.7465, 0.7471, 0.7458]  average = 0.7461\n",
      "\tValidation accuracies: [0.7479, 0.747, 0.7461, 0.7423, 0.7471]  average = 0.7461\n",
      "Pre-processing hyperparameters: {'degree': 2, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7787, 0.7778, 0.7773, 0.7772, 0.778]  average = 0.7778\n",
      "\tValidation accuracies: [0.7744, 0.777, 0.7794, 0.7791, 0.7779]  average = 0.7776\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7901, 0.7903, 0.7904, 0.7898, 0.7905]  average = 0.7902\n",
      "\tValidation accuracies: [0.7888, 0.7888, 0.7891, 0.7917, 0.7916]  average = 0.79\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7931, 0.7664, 0.7964, 0.7941, 0.7963]  average = 0.7893\n",
      "\tValidation accuracies: [0.7901, 0.7677, 0.7972, 0.7942, 0.7964]  average = 0.7891\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.7964, 0.7975, 0.7985, 0.8, 0.8003]  average = 0.7985\n",
      "\tValidation accuracies: [0.7971, 0.8022, 0.7983, 0.7962, 0.7966]  average = 0.7981\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 12}\n",
      "\tTraining accuracies:   [0.539, 0.7668, 0.7608, 0.5512, 0.8036]  average = 0.6843\n",
      "\tValidation accuracies: [0.5387, 0.7653, 0.7604, 0.5518, 0.8009]  average = 0.6834\n",
      "Optimal pre-processing parameters: {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "Optimal average validation accuracy: 0.8074\n",
      "Average training accuracy without outliers: 0.808\n",
      "Optimal pre-processing parameters:  {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "Training accuracy with outliers:  0.7269\n"
     ]
    }
   ],
   "source": [
    "model_name = least_squares\n",
    "\n",
    "alphas = [None, 0.5, 1]\n",
    "betas = [None, 5, 8, 10, 12]\n",
    "degrees = [1, 2, 3, 4, 5, 6]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "P = len(preHyps)\n",
    "avgValAccs = np.zeros(P)\n",
    "avgTrainAccs = np.zeros(P)\n",
    "\n",
    "for p in range(P):\n",
    "    preHyp = preHyps[p]\n",
    "    print(\"Pre-processing hyperparameters:\", str(preHyp))\n",
    "    pre_x, pre_y = pre_process(x=train_data, y=train_labels, **preHyp)\n",
    "    valAccs, trainAccs = kfoldCV(pre_x, pre_y, K, model_name, {})\n",
    "    avgValAccs[p] = avg(valAccs)\n",
    "    avgTrainAccs[p] = avg(trainAccs)\n",
    "    if verbose: \n",
    "        #print(\"Pre-processing hyperparameters:\", str(preHyp))\n",
    "        print('\\tTraining accuracies:  ', trainAccs, ' average =', avg(trainAccs))\n",
    "        print('\\tValidation accuracies:', valAccs, ' average =', avg(valAccs))    \n",
    "\n",
    "optP = np.argmax(avgValAccs)\n",
    "optPreHypLS = preHyps[optP]\n",
    "optValAcc = avgValAccs[optP]\n",
    "print('Optimal pre-processing parameters:', optPreHypLS)\n",
    "print('Optimal average validation accuracy:', avgValAccs[optP])\n",
    "print('Average training accuracy without outliers:', avgTrainAccs[optP])\n",
    "\n",
    "# Pre-process whole training set with optimal pre-processing hyperparameters\n",
    "print('Optimal pre-processing parameters: ', optPreHypLS)\n",
    "x_train_opt, y_train_opt = pre_process(x=train_data, y=train_labels, **optPreHypLS)\n",
    "\n",
    "# Train model with optimal hyperparameters and whole training set\n",
    "wOpt, _ = least_squares(y=y_train_opt, tx=x_train_opt)\n",
    "\n",
    "# Pre-process whole training set but keep outlier rows\n",
    "fullOptPreHypLS = optPreHypLS\n",
    "fullOptPreHypLS['beta'] = None\n",
    "x_opt_full, _ = pre_process(x=train_data, y=train_labels, **fullOptPreHypLS)\n",
    "\n",
    "# Obtain final prediction for whole training set\n",
    "optPred = prediction(wOpt, x_opt_full)\n",
    "optAccuracy = evaluate_accuracy(optPred, train_labels)\n",
    "print('Training accuracy with outliers: ', optAccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameters, with corresponding validation accuracy and test accuracy**\n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': 5} 0.8049, 0.5007    \n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': 8} 0.8068, 0.6416    MAX VALID\n",
    "\n",
    "With $\\beta = 10$:\n",
    "\n",
    "- {'degree': 2, 'alpha': 1, 'beta': 10} 0.7789, 0.7638\n",
    "\n",
    "- {'degree': 3, 'alpha': 1, 'beta': 10} 0.7914, 0.7883\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': 10}, 0.7964, 0.7900  \n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': 10} 0.7634, 0.7574\n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': 10} 0.8074, 0.7297\n",
    "\n",
    "We select degree 4, run additional tests:\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': 8} 0.7972, 0.7602\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': 9}    ?    0.7735 \n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': 11}   ?    0.7904     MAX TEST\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': 12} 0.7891, 0.7889    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy(least_squares, {'degree': 4, 'alpha': 1, 'beta': None}, {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above experiment, we observed that we obtained higher accuracy with the pre-processing hyperparameters: $\\alpha = 1, \\beta \\in \\{None, 5, 10\\}$ and $4 \\leq d \\leq 7.$ \n",
    "Additionally, we observe that low values of $\\beta$ tend to decrease the test accuracy. For the remaining experiments, we therefore will only use $\\alpha = 1$ and optimize over the aforementioned values for $\\beta$ and $d$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Ridge Regression\n",
    "\n",
    "We now train ridge regression, which is simply least squares regression with a regularizer term $\\lambda$ on the $L_2$ norm of the weights $w$, thus computing weights $w^* = (X^T X + \\lambda' I)^{-1} X^T y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7931, 0.7918, 0.7931, 0.7925, 0.7937] \taverage = 0.7928\n",
      "\t\tValidation accuracies:  [0.7921, 0.7944, 0.792, 0.7945, 0.7898] \taverage = 0.7926\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7916, 0.7927, 0.7939, 0.7927, 0.7915] \taverage = 0.7925\n",
      "\t\tValidation accuracies:  [0.7958, 0.7896, 0.7906, 0.79, 0.7942] \taverage = 0.792\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7905, 0.7901, 0.7901, 0.7906, 0.7917] \taverage = 0.7906\n",
      "\t\tValidation accuracies:  [0.7911, 0.7901, 0.7939, 0.7895, 0.787] \taverage = 0.7903\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7848, 0.7843, 0.7844, 0.7836, 0.7846] \taverage = 0.7843\n",
      "\t\tValidation accuracies:  [0.7831, 0.784, 0.7836, 0.7866, 0.7831] \taverage = 0.7841\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7756, 0.7749, 0.7748, 0.7749, 0.7754] \taverage = 0.7751\n",
      "\t\tValidation accuracies:  [0.7734, 0.7759, 0.776, 0.7765, 0.7719] \taverage = 0.7747\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7594, 0.76, 0.759, 0.7596, 0.7589] \taverage = 0.7594\n",
      "\t\tValidation accuracies:  [0.7606, 0.7584, 0.7584, 0.7577, 0.7615] \taverage = 0.7593\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7948, 0.7969, 0.7973, 0.796, 0.796] \taverage = 0.7962\n",
      "\t\tValidation accuracies:  [0.8014, 0.7936, 0.7933, 0.797, 0.7947] \taverage = 0.796\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7954, 0.7956, 0.7951, 0.7956, 0.7949] \taverage = 0.7953\n",
      "\t\tValidation accuracies:  [0.7953, 0.7936, 0.7967, 0.7926, 0.7966] \taverage = 0.795\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7914, 0.7917, 0.7918, 0.7922, 0.7919] \taverage = 0.7918\n",
      "\t\tValidation accuracies:  [0.7934, 0.7916, 0.7924, 0.7904, 0.7903] \taverage = 0.7916\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7864, 0.786, 0.7857, 0.786, 0.786] \taverage = 0.786\n",
      "\t\tValidation accuracies:  [0.7844, 0.7848, 0.7887, 0.7849, 0.7854] \taverage = 0.7856\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7752, 0.7763, 0.7747, 0.7763, 0.7756] \taverage = 0.7756\n",
      "\t\tValidation accuracies:  [0.775, 0.7724, 0.7802, 0.7728, 0.7766] \taverage = 0.7754\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7598, 0.7596, 0.7598, 0.7593, 0.7608] \taverage = 0.7599\n",
      "\t\tValidation accuracies:  [0.7593, 0.7614, 0.7607, 0.7622, 0.7557] \taverage = 0.7599\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7972, 0.798, 0.7978, 0.797, 0.7988] \taverage = 0.7978\n",
      "\t\tValidation accuracies:  [0.7997, 0.798, 0.7976, 0.798, 0.7936] \taverage = 0.7974\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7954, 0.7957, 0.7966, 0.7962, 0.7958] \taverage = 0.7959\n",
      "\t\tValidation accuracies:  [0.7974, 0.7969, 0.7939, 0.7942, 0.7966] \taverage = 0.7958\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7929, 0.7926, 0.7935, 0.7933, 0.7922] \taverage = 0.7929\n",
      "\t\tValidation accuracies:  [0.7922, 0.7934, 0.7904, 0.7917, 0.795] \taverage = 0.7925\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7867, 0.7863, 0.7866, 0.7862, 0.7869] \taverage = 0.7865\n",
      "\t\tValidation accuracies:  [0.7863, 0.7861, 0.786, 0.7867, 0.7855] \taverage = 0.7861\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7766, 0.7772, 0.7762, 0.7763, 0.7772] \taverage = 0.7767\n",
      "\t\tValidation accuracies:  [0.778, 0.7748, 0.7775, 0.7781, 0.7733] \taverage = 0.7763\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7597, 0.7607, 0.7604, 0.7595, 0.7601] \taverage = 0.7601\n",
      "\t\tValidation accuracies:  [0.7611, 0.758, 0.7584, 0.7605, 0.7606] \taverage = 0.7597\n",
      "Pre-processing hyperparameters: {'degree': 7, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7986, 0.7971, 0.7982, 0.7982, 0.7984] \taverage = 0.7981\n",
      "\t\tValidation accuracies:  [0.7931, 0.8008, 0.7956, 0.8, 0.7964] \taverage = 0.7972\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7967, 0.7958, 0.797, 0.7963, 0.7968] \taverage = 0.7965\n",
      "\t\tValidation accuracies:  [0.7946, 0.7988, 0.7947, 0.7961, 0.7944] \taverage = 0.7957\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7934, 0.7933, 0.7937, 0.7939, 0.7936] \taverage = 0.7936\n",
      "\t\tValidation accuracies:  [0.7952, 0.794, 0.7917, 0.7919, 0.7924] \taverage = 0.793\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7879, 0.7875, 0.7868, 0.7858, 0.7864] \taverage = 0.7869\n",
      "\t\tValidation accuracies:  [0.7843, 0.7856, 0.785, 0.7891, 0.7876] \taverage = 0.7863\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7781, 0.7769, 0.7774, 0.7772, 0.7779] \taverage = 0.7775\n",
      "\t\tValidation accuracies:  [0.7753, 0.7801, 0.7761, 0.7784, 0.7761] \taverage = 0.7772\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7599, 0.7603, 0.7595, 0.76, 0.7603] \taverage = 0.76\n",
      "\t\tValidation accuracies:  [0.7617, 0.7588, 0.7608, 0.7597, 0.7586] \taverage = 0.7599\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7937, 0.7933, 0.7936, 0.7931, 0.7914] \taverage = 0.793\n",
      "\t\tValidation accuracies:  [0.7894, 0.7912, 0.7936, 0.7899, 0.7974] \taverage = 0.7923\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7921, 0.793, 0.7925, 0.7923, 0.7923] \taverage = 0.7924\n",
      "\t\tValidation accuracies:  [0.7934, 0.7919, 0.7904, 0.794, 0.7895] \taverage = 0.7918\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7904, 0.7899, 0.7903, 0.791, 0.791] \taverage = 0.7905\n",
      "\t\tValidation accuracies:  [0.7916, 0.7912, 0.791, 0.7886, 0.7886] \taverage = 0.7902\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7846, 0.7848, 0.7835, 0.7847, 0.7846] \taverage = 0.7844\n",
      "\t\tValidation accuracies:  [0.7844, 0.7844, 0.7857, 0.782, 0.7833] \taverage = 0.784\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7753, 0.7747, 0.7751, 0.775, 0.7752] \taverage = 0.7751\n",
      "\t\tValidation accuracies:  [0.7748, 0.778, 0.7733, 0.7736, 0.7746] \taverage = 0.7749\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7587, 0.7603, 0.7597, 0.7592, 0.7593] \taverage = 0.7594\n",
      "\t\tValidation accuracies:  [0.7617, 0.7569, 0.7594, 0.7574, 0.7607] \taverage = 0.7592\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7956, 0.7977, 0.7957, 0.7958, 0.7964] \taverage = 0.7962\n",
      "\t\tValidation accuracies:  [0.7978, 0.7911, 0.7977, 0.7975, 0.7953] \taverage = 0.7959\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7962, 0.7948, 0.7954, 0.7952, 0.795] \taverage = 0.7953\n",
      "\t\tValidation accuracies:  [0.7918, 0.7967, 0.7939, 0.7946, 0.797] \taverage = 0.7948\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7912, 0.7917, 0.7924, 0.7916, 0.7923] \taverage = 0.7918\n",
      "\t\tValidation accuracies:  [0.7935, 0.7905, 0.7911, 0.7933, 0.7884] \taverage = 0.7914\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7855, 0.7861, 0.7862, 0.7861, 0.7864] \taverage = 0.7861\n",
      "\t\tValidation accuracies:  [0.7883, 0.786, 0.7853, 0.7852, 0.7837] \taverage = 0.7857\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7754, 0.776, 0.775, 0.7762, 0.7757] \taverage = 0.7757\n",
      "\t\tValidation accuracies:  [0.7748, 0.7754, 0.7794, 0.7737, 0.7744] \taverage = 0.7755\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7605, 0.7598, 0.7595, 0.7599, 0.76] \taverage = 0.7599\n",
      "\t\tValidation accuracies:  [0.7568, 0.7604, 0.7638, 0.7592, 0.7581] \taverage = 0.7597\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7979, 0.7981, 0.7968, 0.7978, 0.7985] \taverage = 0.7978\n",
      "\t\tValidation accuracies:  [0.799, 0.7948, 0.801, 0.7972, 0.7938] \taverage = 0.7972\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7954, 0.7963, 0.7965, 0.7964, 0.7955] \taverage = 0.796\n",
      "\t\tValidation accuracies:  [0.7983, 0.7943, 0.7934, 0.7948, 0.7975] \taverage = 0.7957\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7924, 0.7929, 0.7933, 0.7926, 0.7929] \taverage = 0.7928\n",
      "\t\tValidation accuracies:  [0.7948, 0.7909, 0.7921, 0.7928, 0.7922] \taverage = 0.7926\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7865, 0.7861, 0.7867, 0.7877, 0.7855] \taverage = 0.7865\n",
      "\t\tValidation accuracies:  [0.7867, 0.7864, 0.7874, 0.7815, 0.788] \taverage = 0.786\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7761, 0.7764, 0.7773, 0.7769, 0.7768] \taverage = 0.7767\n",
      "\t\tValidation accuracies:  [0.779, 0.7776, 0.774, 0.7751, 0.7763] \taverage = 0.7764\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7602, 0.7593, 0.7605, 0.7602, 0.7604] \taverage = 0.7601\n",
      "\t\tValidation accuracies:  [0.7594, 0.7619, 0.7597, 0.7597, 0.758] \taverage = 0.7597\n",
      "Pre-processing hyperparameters: {'degree': 7, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7979, 0.7979, 0.7982, 0.7979, 0.7987] \taverage = 0.7981\n",
      "\t\tValidation accuracies:  [0.7986, 0.797, 0.7968, 0.801, 0.7952] \taverage = 0.7977\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7964, 0.797, 0.797, 0.7964, 0.796] \taverage = 0.7966\n",
      "\t\tValidation accuracies:  [0.7949, 0.7972, 0.7924, 0.7962, 0.7995] \taverage = 0.796\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7941, 0.7937, 0.7926, 0.794, 0.7931] \taverage = 0.7935\n",
      "\t\tValidation accuracies:  [0.7896, 0.7919, 0.7958, 0.792, 0.7962] \taverage = 0.7931\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7868, 0.7866, 0.7867, 0.7867, 0.7875] \taverage = 0.7869\n",
      "\t\tValidation accuracies:  [0.7869, 0.7855, 0.7878, 0.7877, 0.7854] \taverage = 0.7867\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7769, 0.7778, 0.7771, 0.778, 0.7774] \taverage = 0.7774\n",
      "\t\tValidation accuracies:  [0.7772, 0.7751, 0.7789, 0.7746, 0.7788] \taverage = 0.7769\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7604, 0.7603, 0.7599, 0.7602, 0.7598] \taverage = 0.7601\n",
      "\t\tValidation accuracies:  [0.7603, 0.7574, 0.7604, 0.7605, 0.7594] \taverage = 0.7596\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7921, 0.7923, 0.7932, 0.7936, 0.7933] \taverage = 0.7929\n",
      "\t\tValidation accuracies:  [0.7939, 0.7928, 0.791, 0.7917, 0.7932] \taverage = 0.7925\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7928, 0.7922, 0.7916, 0.7926, 0.7928] \taverage = 0.7924\n",
      "\t\tValidation accuracies:  [0.7953, 0.7909, 0.7946, 0.7892, 0.791] \taverage = 0.7922\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7906, 0.7898, 0.7907, 0.7908, 0.7909] \taverage = 0.7906\n",
      "\t\tValidation accuracies:  [0.7891, 0.7928, 0.7895, 0.7902, 0.7898] \taverage = 0.7903\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7839, 0.7844, 0.7843, 0.7851, 0.7843] \taverage = 0.7844\n",
      "\t\tValidation accuracies:  [0.785, 0.7845, 0.7852, 0.7814, 0.7844] \taverage = 0.7841\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7753, 0.7748, 0.7749, 0.7751, 0.7754] \taverage = 0.7751\n",
      "\t\tValidation accuracies:  [0.7736, 0.776, 0.7749, 0.7734, 0.7754] \taverage = 0.7747\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7595, 0.7584, 0.7599, 0.7596, 0.7599] \taverage = 0.7595\n",
      "\t\tValidation accuracies:  [0.7601, 0.7628, 0.7573, 0.7572, 0.7586] \taverage = 0.7592\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.797, 0.7965, 0.7957, 0.7968, 0.7951] \taverage = 0.7962\n",
      "\t\tValidation accuracies:  [0.7942, 0.7932, 0.7981, 0.7931, 0.8002] \taverage = 0.7958\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7952, 0.7953, 0.7953, 0.7955, 0.7951] \taverage = 0.7953\n",
      "\t\tValidation accuracies:  [0.7944, 0.7939, 0.7977, 0.7927, 0.7948] \taverage = 0.7947\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.792, 0.7915, 0.7915, 0.7925, 0.7914] \taverage = 0.7918\n",
      "\t\tValidation accuracies:  [0.7906, 0.792, 0.793, 0.7882, 0.7935] \taverage = 0.7915\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7863, 0.7853, 0.7856, 0.7871, 0.7861] \taverage = 0.7861\n",
      "\t\tValidation accuracies:  [0.7839, 0.7884, 0.7886, 0.7817, 0.7853] \taverage = 0.7856\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7759, 0.7755, 0.776, 0.7752, 0.7757] \taverage = 0.7757\n",
      "\t\tValidation accuracies:  [0.7738, 0.7754, 0.7754, 0.7762, 0.7765] \taverage = 0.7755\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7591, 0.7602, 0.7602, 0.7598, 0.76] \taverage = 0.7599\n",
      "\t\tValidation accuracies:  [0.7612, 0.7577, 0.7604, 0.76, 0.7588] \taverage = 0.7596\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7974, 0.7982, 0.7982, 0.7972, 0.7977] \taverage = 0.7977\n",
      "\t\tValidation accuracies:  [0.797, 0.7935, 0.7993, 0.799, 0.797] \taverage = 0.7972\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7962, 0.796, 0.7958, 0.7968, 0.7951] \taverage = 0.796\n",
      "\t\tValidation accuracies:  [0.794, 0.7959, 0.7942, 0.7932, 0.7996] \taverage = 0.7954\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.7931, 0.7935, 0.7932, 0.7914, 0.7934] \taverage = 0.7929\n",
      "\t\tValidation accuracies:  [0.792, 0.7926, 0.7906, 0.7974, 0.7896] \taverage = 0.7924\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7874, 0.7864, 0.7862, 0.7863, 0.7863] \taverage = 0.7865\n",
      "\t\tValidation accuracies:  [0.7839, 0.7862, 0.7852, 0.7852, 0.79] \taverage = 0.7861\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7771, 0.7766, 0.7768, 0.776, 0.7766] \taverage = 0.7766\n",
      "\t\tValidation accuracies:  [0.7737, 0.7757, 0.7768, 0.7792, 0.7762] \taverage = 0.7763\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7595, 0.7598, 0.7602, 0.7597, 0.7611] \taverage = 0.7601\n",
      "\t\tValidation accuracies:  [0.7625, 0.7611, 0.7592, 0.7617, 0.7545] \taverage = 0.7598\n",
      "Pre-processing hyperparameters: {'degree': 7, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'lambda_': 1e-06}\n",
      "\t\tTraining accuracies:  [0.7976, 0.7983, 0.7979, 0.7991, 0.798] \taverage = 0.7982\n",
      "\t\tValidation accuracies:  [0.8003, 0.7956, 0.7975, 0.7966, 0.7964] \taverage = 0.7973\n",
      "\tModel hyperparameters: {'lambda_': 1e-05}\n",
      "\t\tTraining accuracies:  [0.7966, 0.7964, 0.7957, 0.7966, 0.7971] \taverage = 0.7965\n",
      "\t\tValidation accuracies:  [0.7958, 0.7963, 0.7974, 0.795, 0.7957] \taverage = 0.796\n",
      "\tModel hyperparameters: {'lambda_': 0.0001}\n",
      "\t\tTraining accuracies:  [0.794, 0.7937, 0.7935, 0.7929, 0.7935] \taverage = 0.7935\n",
      "\t\tValidation accuracies:  [0.7917, 0.794, 0.7927, 0.795, 0.7932] \taverage = 0.7933\n",
      "\tModel hyperparameters: {'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7867, 0.787, 0.7869, 0.7871, 0.7864] \taverage = 0.7868\n",
      "\t\tValidation accuracies:  [0.7859, 0.7863, 0.7882, 0.7844, 0.7879] \taverage = 0.7865\n",
      "\tModel hyperparameters: {'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7782, 0.7773, 0.7776, 0.7769, 0.7771] \taverage = 0.7774\n",
      "\t\tValidation accuracies:  [0.7731, 0.7771, 0.7759, 0.7802, 0.78] \taverage = 0.7773\n",
      "\tModel hyperparameters: {'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7601, 0.7595, 0.7606, 0.7601, 0.7599] \taverage = 0.76\n",
      "\t\tValidation accuracies:  [0.7588, 0.7607, 0.7568, 0.7608, 0.7612] \taverage = 0.7597\n",
      "Optimal pre-processing parameters:  {'degree': 7, 'alpha': 1, 'beta': None}\n",
      "Optimal model parameters:  {'lambda_': 1e-06}\n",
      "Average K-fold validation accuracy: 0.7977\n",
      "Average K-fold training accuracy: 0.7981\n",
      "Full training accuracy for optimal hyperparameters:  0.7979\n"
     ]
    }
   ],
   "source": [
    "model_name = ridge_regression\n",
    "\n",
    "alphas = [1]\n",
    "betas = [None, 5, 10]\n",
    "degrees = [4, 5, 6, 7]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "lambdas = [1e-06, 1e-05, 1e-04, 0.001, 0.01, 0.1]\n",
    "hypsRR = [{'lambda_': l} for l in lambdas]\n",
    "\n",
    "optPreHypRR, optHypRR, optAccuracyRR = hyperparameterTuning(train_data, train_labels, K, model_name, preHyps, hypsRR, verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameters, with corresponding validation accuracy and test accuracy**\n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': None} {'lambda_': 1e-06}  0.7959  0.7892\n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': 5} {'lambda_': 1e-06}     0.7959  0.683\n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': 10} {'lambda_': 1e-06}    0.7956  0.7829\n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': None} {'lambda_': 1e-06}  0.7974  0.7904        MAX TEST\n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': 5} {'lambda_': 1e-06}     0.7972  0.6821\n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': 10} {'lambda_': 1e-06}    0.7972  0.7725\n",
    "\n",
    "- {'degree': 7, 'alpha': 1, 'beta': None} {'lambda_': 1e-06}  0.7977  0.7898        MAX VALID\n",
    "\n",
    "- {'degree': 7, 'alpha': 1, 'beta': 5} {'lambda_': 1e-06}     0.7976  0.6714\n",
    "\n",
    "- {'degree': 7, 'alpha': 1, 'beta': 10} {'lambda_': 1e-06}    0.7974  0.7566\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.783\n"
     ]
    }
   ],
   "source": [
    "testAccRR = testAccuracy(ridge_regression, {'degree': 7, 'alpha': 1, 'beta': None}, {'lambda_': 1e-05})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ridge regression, we observe that the optimal validation accuracy is always obtained when the regularization parameter is $\\lambda = 0.000001$, which corresponds to the lowest value over which we optimize. The lower the $\\lambda$, the closer ridge regression comes to the original least squares model. Additionally, none of the validation or test accuracies go over that of least squares. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Linear Regression GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7202, 0.7194, 0.7185, 0.7199, 0.7193] \taverage = 0.7195\n",
      "\t\tValidation accuracies:  [0.7145, 0.7209, 0.7236, 0.7186, 0.7194] \taverage = 0.7194\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7439, 0.7442, 0.7448, 0.7456, 0.744] \taverage = 0.7445\n",
      "\t\tValidation accuracies:  [0.7465, 0.7455, 0.7448, 0.7393, 0.746] \taverage = 0.7444\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7524, 0.7513, 0.7514, 0.7512, 0.7524] \taverage = 0.7517\n",
      "\t\tValidation accuracies:  [0.7486, 0.7541, 0.7522, 0.755, 0.7483] \taverage = 0.7516\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7685, 0.7685, 0.7685, 0.7685, 0.7688] \taverage = 0.7686\n",
      "\t\tValidation accuracies:  [0.7692, 0.7676, 0.7671, 0.7698, 0.7682] \taverage = 0.7684\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7726, 0.7723, 0.7734, 0.7735, 0.7725] \taverage = 0.7729\n",
      "\t\tValidation accuracies:  [0.7748, 0.7742, 0.7712, 0.7678, 0.774] \taverage = 0.7724\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7171, 0.7171, 0.7178, 0.7173, 0.7169] \taverage = 0.7172\n",
      "\t\tValidation accuracies:  [0.717, 0.7161, 0.7158, 0.7176, 0.7194] \taverage = 0.7172\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7432, 0.7438, 0.743, 0.7429, 0.7437] \taverage = 0.7433\n",
      "\t\tValidation accuracies:  [0.7435, 0.7431, 0.746, 0.7426, 0.7399] \taverage = 0.743\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7525, 0.7516, 0.752, 0.7533, 0.7528] \taverage = 0.7524\n",
      "\t\tValidation accuracies:  [0.753, 0.7564, 0.7518, 0.75, 0.7508] \taverage = 0.7524\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7685, 0.7683, 0.7696, 0.769, 0.769] \taverage = 0.7689\n",
      "\t\tValidation accuracies:  [0.7714, 0.7712, 0.7645, 0.7671, 0.769] \taverage = 0.7686\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.358, 0.7723, 0.3605, 0.3599, 0.3582] \taverage = 0.4418\n",
      "\t\tValidation accuracies:  [0.3602, 0.7731, 0.3584, 0.3563, 0.3581] \taverage = 0.4412\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7166, 0.7167, 0.7169, 0.7164, 0.7175] \taverage = 0.7168\n",
      "\t\tValidation accuracies:  [0.7174, 0.7164, 0.7154, 0.719, 0.715] \taverage = 0.7166\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.743, 0.7433, 0.7432, 0.7424, 0.7427] \taverage = 0.7429\n",
      "\t\tValidation accuracies:  [0.7424, 0.7418, 0.7429, 0.7437, 0.743] \taverage = 0.7428\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7524, 0.7523, 0.7534, 0.7521, 0.7531] \taverage = 0.7527\n",
      "\t\tValidation accuracies:  [0.7534, 0.7553, 0.75, 0.7524, 0.7518] \taverage = 0.7526\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7688, 0.768, 0.7687, 0.7686, 0.768] \taverage = 0.7684\n",
      "\t\tValidation accuracies:  [0.7637, 0.772, 0.7664, 0.7686, 0.7708] \taverage = 0.7683\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.3584, 0.3579, 0.3587, 0.3587, 0.3583] \taverage = 0.3584\n",
      "\t\tValidation accuracies:  [0.3585, 0.3601, 0.3574, 0.3579, 0.3578] \taverage = 0.3583\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7203, 0.7196, 0.7191, 0.7186, 0.7195] \taverage = 0.7194\n",
      "\t\tValidation accuracies:  [0.7161, 0.7184, 0.7215, 0.7206, 0.7209] \taverage = 0.7195\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7443, 0.7442, 0.7443, 0.745, 0.7449] \taverage = 0.7445\n",
      "\t\tValidation accuracies:  [0.7439, 0.7455, 0.746, 0.7427, 0.744] \taverage = 0.7444\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7512, 0.7515, 0.7515, 0.7522, 0.7522] \taverage = 0.7517\n",
      "\t\tValidation accuracies:  [0.7538, 0.7527, 0.7529, 0.7499, 0.7488] \taverage = 0.7516\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7687, 0.7681, 0.7685, 0.7682, 0.7691] \taverage = 0.7685\n",
      "\t\tValidation accuracies:  [0.7676, 0.7708, 0.769, 0.7686, 0.7661] \taverage = 0.7684\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7733, 0.7729, 0.7731, 0.7723, 0.7728] \taverage = 0.7729\n",
      "\t\tValidation accuracies:  [0.771, 0.7739, 0.7709, 0.7739, 0.7727] \taverage = 0.7725\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7181, 0.7174, 0.7168, 0.7167, 0.7173] \taverage = 0.7173\n",
      "\t\tValidation accuracies:  [0.712, 0.7173, 0.7183, 0.7201, 0.7182] \taverage = 0.7172\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.743, 0.7433, 0.7437, 0.7434, 0.7432] \taverage = 0.7433\n",
      "\t\tValidation accuracies:  [0.7446, 0.742, 0.7417, 0.7446, 0.7427] \taverage = 0.7431\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7531, 0.7531, 0.7526, 0.7521, 0.7515] \taverage = 0.7525\n",
      "\t\tValidation accuracies:  [0.7488, 0.7521, 0.7537, 0.7519, 0.7543] \taverage = 0.7522\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7686, 0.7687, 0.7695, 0.7689, 0.7685] \taverage = 0.7688\n",
      "\t\tValidation accuracies:  [0.7676, 0.7703, 0.7674, 0.7688, 0.7686] \taverage = 0.7685\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.3594, 0.7416, 0.3622, 0.3593, 0.441] \taverage = 0.4527\n",
      "\t\tValidation accuracies:  [0.3574, 0.7384, 0.3657, 0.3571, 0.4418] \taverage = 0.4521\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7165, 0.7178, 0.7172, 0.7165, 0.7162] \taverage = 0.7168\n",
      "\t\tValidation accuracies:  [0.7179, 0.7126, 0.7153, 0.7184, 0.7194] \taverage = 0.7167\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7435, 0.743, 0.7428, 0.7427, 0.7425] \taverage = 0.7429\n",
      "\t\tValidation accuracies:  [0.742, 0.743, 0.7409, 0.7427, 0.7452] \taverage = 0.7428\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7527, 0.7533, 0.7527, 0.7528, 0.7522] \taverage = 0.7527\n",
      "\t\tValidation accuracies:  [0.7535, 0.7506, 0.7531, 0.7507, 0.7538] \taverage = 0.7523\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7684, 0.7685, 0.7688, 0.7683, 0.7684] \taverage = 0.7685\n",
      "\t\tValidation accuracies:  [0.77, 0.7673, 0.7657, 0.7692, 0.7681] \taverage = 0.7681\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.359, 0.3586, 0.3579, 0.3582, 0.3583] \taverage = 0.3584\n",
      "\t\tValidation accuracies:  [0.356, 0.358, 0.3587, 0.3605, 0.3585] \taverage = 0.3583\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7182, 0.7192, 0.7193, 0.7199, 0.7204] \taverage = 0.7194\n",
      "\t\tValidation accuracies:  [0.7248, 0.7182, 0.7208, 0.7157, 0.7176] \taverage = 0.7194\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7451, 0.7444, 0.7438, 0.7454, 0.7441] \taverage = 0.7446\n",
      "\t\tValidation accuracies:  [0.7443, 0.7442, 0.748, 0.7411, 0.7458] \taverage = 0.7447\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.752, 0.7512, 0.7518, 0.7514, 0.7521] \taverage = 0.7517\n",
      "\t\tValidation accuracies:  [0.7512, 0.7512, 0.751, 0.7506, 0.7533] \taverage = 0.7515\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7686, 0.7686, 0.7685, 0.7688, 0.7682] \taverage = 0.7685\n",
      "\t\tValidation accuracies:  [0.7674, 0.7687, 0.7682, 0.7668, 0.7707] \taverage = 0.7684\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7735, 0.7722, 0.7737, 0.7724, 0.7728] \taverage = 0.7729\n",
      "\t\tValidation accuracies:  [0.7707, 0.775, 0.7701, 0.7737, 0.7738] \taverage = 0.7727\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7175, 0.718, 0.7168, 0.717, 0.7168] \taverage = 0.7172\n",
      "\t\tValidation accuracies:  [0.7162, 0.7116, 0.7199, 0.721, 0.7172] \taverage = 0.7172\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7439, 0.7428, 0.7436, 0.7437, 0.7428] \taverage = 0.7434\n",
      "\t\tValidation accuracies:  [0.7419, 0.7423, 0.7438, 0.7433, 0.7442] \taverage = 0.7431\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.753, 0.7525, 0.7521, 0.753, 0.7517] \taverage = 0.7525\n",
      "\t\tValidation accuracies:  [0.7511, 0.7532, 0.7528, 0.7501, 0.7547] \taverage = 0.7524\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.769, 0.7686, 0.7687, 0.7691, 0.7687] \taverage = 0.7688\n",
      "\t\tValidation accuracies:  [0.7677, 0.7704, 0.7686, 0.7667, 0.7696] \taverage = 0.7686\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.3586, 0.773, 0.773, 0.3585, 0.3588] \taverage = 0.5244\n",
      "\t\tValidation accuracies:  [0.3614, 0.7729, 0.7704, 0.3594, 0.3575] \taverage = 0.5243\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.001}\n",
      "\t\tTraining accuracies:  [0.7163, 0.7167, 0.717, 0.7172, 0.7166] \taverage = 0.7168\n",
      "\t\tValidation accuracies:  [0.7186, 0.718, 0.7141, 0.7161, 0.7166] \taverage = 0.7167\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.005}\n",
      "\t\tTraining accuracies:  [0.7432, 0.7436, 0.7425, 0.7423, 0.7429] \taverage = 0.7429\n",
      "\t\tValidation accuracies:  [0.7428, 0.741, 0.7437, 0.7452, 0.7409] \taverage = 0.7427\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7525, 0.7518, 0.7528, 0.7533, 0.753] \taverage = 0.7527\n",
      "\t\tValidation accuracies:  [0.7542, 0.7555, 0.7502, 0.7494, 0.7533] \taverage = 0.7525\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7685, 0.769, 0.7678, 0.7686, 0.7687] \taverage = 0.7685\n",
      "\t\tValidation accuracies:  [0.7683, 0.7658, 0.7696, 0.7677, 0.7687] \taverage = 0.768\n",
      "\tModel hyperparameters: {'max_iters': 200, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.3585, 0.3579, 0.3581, 0.3594, 0.3582] \taverage = 0.3584\n",
      "\t\tValidation accuracies:  [0.3594, 0.3594, 0.3594, 0.3552, 0.359] \taverage = 0.3585\n",
      "Optimal pre-processing parameters:  {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "Optimal model parameters:  {'max_iters': 200, 'gamma': 0.1}\n",
      "Average K-fold validation accuracy: 0.7727\n",
      "Average K-fold training accuracy: 0.7729\n",
      "Full training accuracy for optimal hyperparameters:  0.7727\n"
     ]
    }
   ],
   "source": [
    "model_name = mean_squared_error_gd\n",
    "\n",
    "alphas = [1]\n",
    "betas = [None, 5, 10]\n",
    "degrees = [4, 5, 6]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "maxs_iters = [200]\n",
    "gammas = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "hypsGD = [{'max_iters': mi, 'gamma': g} for g in gammas for mi in maxs_iters]\n",
    "\n",
    "optPreHypGD, optHypGD, optAccuracyGD = hyperparameterTuning(train_data, train_labels, K, model_name, preHyps, hypsGD, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameters, with corresponding validation accuracy and test accuracy**\n",
    "\n",
    "{'degree': 4, 'alpha': 1, 'beta': None} {'max_iters': 200, 'gamma': 0.1}    0.7724  0.7733\n",
    "\n",
    "{'degree': 4, 'alpha': 1, 'beta': 5} {'max_iters': 200, 'gamma': 0.1}       0.7725  0.3605\n",
    "\n",
    "{'degree': 4, 'alpha': 1, 'beta': 10} {'max_iters': 200, 'gamma': 0.1}      0.7727  0.3609\n",
    "\n",
    "{'degree': 4, 'alpha': 1, 'beta': None} {'max_iters': 500, 'gamma': 0.1}    0.7724  0.7757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Accuracy**\n",
    "\n",
    "Test accuracy of hyperparameters with highest validation accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7757\n"
     ]
    }
   ],
   "source": [
    "testAccGD = testAccuracy(mean_squared_error_gd, {'degree': 4, 'alpha': 1, 'beta': None}, {'max_iters': 500, 'gamma': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy of other hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAccuracy(model_name, {'degree': 4, 'alpha': 1, 'beta': None} , {'max_iters': 200, 'gamma': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Linear Regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.6975, 0.6011, 0.7232, 0.6847, 0.6858] \taverage = 0.6785\n",
      "\t\tValidation accuracies:  [0.6992, 0.603, 0.7193, 0.6814, 0.6854] \taverage = 0.6777\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7178, 0.7235, 0.7121, 0.7214, 0.7169] \taverage = 0.7183\n",
      "\t\tValidation accuracies:  [0.722, 0.719, 0.7137, 0.7202, 0.7168] \taverage = 0.7183\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.724, 0.7194, 0.7274, 0.726, 0.7253] \taverage = 0.7244\n",
      "\t\tValidation accuracies:  [0.7244, 0.724, 0.7229, 0.7202, 0.7305] \taverage = 0.7244\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.7256, 0.7202, 0.7208, 0.7226, 0.7222] \taverage = 0.7223\n",
      "\t\tValidation accuracies:  [0.724, 0.7231, 0.7233, 0.7199, 0.7194] \taverage = 0.7219\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.6181, 0.4989, 0.4684, 0.5989, 0.4704] \taverage = 0.5309\n",
      "\t\tValidation accuracies:  [0.6151, 0.497, 0.4687, 0.6035, 0.4742] \taverage = 0.5317\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7266, 0.736, 0.7262, 0.7441, 0.7511] \taverage = 0.7368\n",
      "\t\tValidation accuracies:  [0.7274, 0.7335, 0.7295, 0.7413, 0.7544] \taverage = 0.7372\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.6779, 0.758, 0.7391, 0.7514, 0.7534] \taverage = 0.736\n",
      "\t\tValidation accuracies:  [0.6798, 0.7608, 0.7397, 0.7526, 0.7497] \taverage = 0.7365\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.7516, 0.7517, 0.7527, 0.7526, 0.7531] \taverage = 0.7523\n",
      "\t\tValidation accuracies:  [0.755, 0.7503, 0.7553, 0.7511, 0.75] \taverage = 0.7523\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.495, 0.5279, 0.4907, 0.4139, 0.6288] \taverage = 0.5113\n",
      "\t\tValidation accuracies:  [0.4967, 0.5254, 0.4878, 0.4142, 0.6289] \taverage = 0.5106\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.4439, 0.5575, 0.3255, 0.588, 0.5617] \taverage = 0.4953\n",
      "\t\tValidation accuracies:  [0.444, 0.5569, 0.3283, 0.5881, 0.5614] \taverage = 0.4957\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.5974, 0.5367, 0.4622, 0.532, 0.5642] \taverage = 0.5385\n",
      "\t\tValidation accuracies:  [0.5983, 0.532, 0.4612, 0.5295, 0.5651] \taverage = 0.5372\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.4929, 0.5573, 0.3736, 0.5819, 0.4381] \taverage = 0.4888\n",
      "\t\tValidation accuracies:  [0.4937, 0.5576, 0.3781, 0.5817, 0.4368] \taverage = 0.4896\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 5}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.6919, 0.7181, 0.7062, 0.7101, 0.7231] \taverage = 0.7099\n",
      "\t\tValidation accuracies:  [0.6921, 0.717, 0.7037, 0.7102, 0.7256] \taverage = 0.7097\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7243, 0.7149, 0.7211, 0.7171, 0.7228] \taverage = 0.72\n",
      "\t\tValidation accuracies:  [0.7198, 0.7166, 0.719, 0.7156, 0.725] \taverage = 0.7192\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.726, 0.7224, 0.7201, 0.7224, 0.7245] \taverage = 0.7231\n",
      "\t\tValidation accuracies:  [0.724, 0.7242, 0.7198, 0.725, 0.7242] \taverage = 0.7234\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.7216, 0.7225, 0.7226, 0.7242, 0.7239] \taverage = 0.723\n",
      "\t\tValidation accuracies:  [0.7183, 0.7224, 0.7258, 0.7233, 0.7251] \taverage = 0.723\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.5757, 0.4695, 0.5377, 0.4956, 0.3637] \taverage = 0.4884\n",
      "\t\tValidation accuracies:  [0.5736, 0.4714, 0.5344, 0.4959, 0.3622] \taverage = 0.4875\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7501, 0.7461, 0.7474, 0.7369, 0.7311] \taverage = 0.7423\n",
      "\t\tValidation accuracies:  [0.7512, 0.7466, 0.7478, 0.7337, 0.7317] \taverage = 0.7422\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.7558, 0.7349, 0.6847, 0.7458, 0.7432] \taverage = 0.7329\n",
      "\t\tValidation accuracies:  [0.757, 0.7372, 0.6826, 0.7476, 0.7454] \taverage = 0.734\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.7552, 0.7544, 0.7543, 0.7562, 0.756] \taverage = 0.7552\n",
      "\t\tValidation accuracies:  [0.7523, 0.7542, 0.7542, 0.7569, 0.7544] \taverage = 0.7544\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.6128, 0.3933, 0.425, 0.5576, 0.4796] \taverage = 0.4937\n",
      "\t\tValidation accuracies:  [0.6148, 0.3963, 0.4256, 0.5558, 0.4796] \taverage = 0.4944\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.6402, 0.483, 0.5126, 0.3476, 0.399] \taverage = 0.4765\n",
      "\t\tValidation accuracies:  [0.6408, 0.4791, 0.5073, 0.3486, 0.3976] \taverage = 0.4747\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.4302, 0.5204, 0.5088, 0.4257, 0.4703] \taverage = 0.4711\n",
      "\t\tValidation accuracies:  [0.4296, 0.5269, 0.5117, 0.4266, 0.4662] \taverage = 0.4722\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.5091, 0.5962, 0.5862, 0.6375, 0.5537] \taverage = 0.5765\n",
      "\t\tValidation accuracies:  [0.5077, 0.592, 0.5879, 0.6364, 0.5517] \taverage = 0.5751\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.7018, 0.7236, 0.7024, 0.7017, 0.6923] \taverage = 0.7044\n",
      "\t\tValidation accuracies:  [0.705, 0.7231, 0.6998, 0.702, 0.686] \taverage = 0.7032\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7188, 0.7184, 0.7211, 0.72, 0.7345] \taverage = 0.7226\n",
      "\t\tValidation accuracies:  [0.7127, 0.7204, 0.7213, 0.7206, 0.7352] \taverage = 0.722\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.7245, 0.722, 0.7228, 0.7236, 0.7233] \taverage = 0.7232\n",
      "\t\tValidation accuracies:  [0.7243, 0.7229, 0.7226, 0.7226, 0.7244] \taverage = 0.7234\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.001, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.723, 0.7229, 0.7227, 0.725, 0.7219] \taverage = 0.7231\n",
      "\t\tValidation accuracies:  [0.7266, 0.7234, 0.7211, 0.7243, 0.7174] \taverage = 0.7226\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.5847, 0.5596, 0.6134, 0.5188, 0.6803] \taverage = 0.5914\n",
      "\t\tValidation accuracies:  [0.5851, 0.5612, 0.6183, 0.5167, 0.6793] \taverage = 0.5921\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.7047, 0.7392, 0.7401, 0.7237, 0.7138] \taverage = 0.7243\n",
      "\t\tValidation accuracies:  [0.7033, 0.736, 0.7388, 0.7256, 0.7162] \taverage = 0.724\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.7437, 0.7519, 0.7508, 0.7506, 0.7514] \taverage = 0.7497\n",
      "\t\tValidation accuracies:  [0.7418, 0.7481, 0.7511, 0.7536, 0.7517] \taverage = 0.7493\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.633, 0.7466, 0.7488, 0.7467, 0.7528] \taverage = 0.7256\n",
      "\t\tValidation accuracies:  [0.6325, 0.7508, 0.7429, 0.7466, 0.7502] \taverage = 0.7246\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 1}\n",
      "\t\tTraining accuracies:  [0.542, 0.5087, 0.6378, 0.505, 0.3387] \taverage = 0.5064\n",
      "\t\tValidation accuracies:  [0.5467, 0.5119, 0.6368, 0.5044, 0.3425] \taverage = 0.5085\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 10}\n",
      "\t\tTraining accuracies:  [0.462, 0.5736, 0.4329, 0.6305, 0.6086] \taverage = 0.5415\n",
      "\t\tValidation accuracies:  [0.4594, 0.5734, 0.4329, 0.6321, 0.6056] \taverage = 0.5407\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 50}\n",
      "\t\tTraining accuracies:  [0.6191, 0.5102, 0.5831, 0.4169, 0.6211] \taverage = 0.5501\n",
      "\t\tValidation accuracies:  [0.6244, 0.5085, 0.5864, 0.4137, 0.6274] \taverage = 0.5521\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1, 'batch_size': 100}\n",
      "\t\tTraining accuracies:  [0.3657, 0.6052, 0.5818, 0.6392, 0.5895] \taverage = 0.5563\n",
      "\t\tValidation accuracies:  [0.3661, 0.6056, 0.5826, 0.643, 0.5913] \taverage = 0.5577\n",
      "Optimal pre-processing parameters:  {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "Optimal model parameters:  {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100}\n",
      "Average K-fold validation accuracy: 0.7544\n",
      "Average K-fold training accuracy: 0.7552\n",
      "Full training accuracy for optimal hyperparameters:  0.7456\n"
     ]
    }
   ],
   "source": [
    "model_name = mean_squared_error_sgd\n",
    "\n",
    "alphas = [1]\n",
    "betas = [None, 5, 10]\n",
    "degrees = [4]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "batch_sizes = [1, 50, 100, 200]\n",
    "maxs_iters = [250]\n",
    "gammas = [0.001, 0.01, 0.1]\n",
    "\n",
    "hypsSGD = [{'max_iters': mi, 'gamma': g, 'batch_size' : b} \n",
    "                     for g in gammas for mi in maxs_iters for b in batch_sizes]\n",
    "\n",
    "optPreHypSGD, optHypSGD, optAccuracySGD = hyperparameterTuning(train_data, train_labels, K, model_name, preHyps, hypsSGD, verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameters**\n",
    "\n",
    "We experiment with some hyperparameters and their test accuracy:\n",
    "\n",
    "{'degree': 4, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100} 0.728\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100} 0.7536\n",
    "\n",
    "{'degree': 6, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100} 0.7457\n",
    "\n",
    "We choose degree 5\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': 5} {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100} 0.7516\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': 10} {'max_iters': 250, 'gamma': 0.01, 'batch_size': 100} 0.7563\n",
    "\n",
    "We choose beta = 10\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': 10} {'max_iters': 250, 'gamma': 0.001, 'batch_size': 100} 0.7221\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': 10} {'max_iters': 250, 'gamma': 0.1, 'batch_size': 100} 0.6542\n",
    "\n",
    "We keep gamma = 0.01\n",
    "\n",
    "{'degree': 5, 'alpha': 1, 'beta': 10} {'max_iters': 500, 'gamma': 0.01, 'batch_size': 100} 0.7627\n",
    "\n",
    "We've found good hyperparameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7627\n"
     ]
    }
   ],
   "source": [
    "testAccSGD = testAccuracy(mean_squared_error_sgd, {'degree': 5, 'alpha': 1, 'beta':10}, {'max_iters': 500, 'gamma': 0.01, 'batch_size': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7287, 0.7279, 0.728, 0.7279, 0.7289] \taverage = 0.7283\n",
      "\t\tValidation accuracies:  [0.7267, 0.7303, 0.7292, 0.7296, 0.7251] \taverage = 0.7282\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7328, 0.7315, 0.7331, 0.7325, 0.7328] \taverage = 0.7325\n",
      "\t\tValidation accuracies:  [0.7318, 0.7357, 0.7308, 0.7319, 0.731] \taverage = 0.7322\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7334, 0.7323, 0.7331, 0.733, 0.7327] \taverage = 0.7329\n",
      "\t\tValidation accuracies:  [0.731, 0.7351, 0.7321, 0.7312, 0.7339] \taverage = 0.7327\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.7327, 0.7339, 0.7335, 0.7333, 0.7324] \taverage = 0.7332\n",
      "\t\tValidation accuracies:  [0.7334, 0.7299, 0.7322, 0.7328, 0.7364] \taverage = 0.7329\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7291, 0.73, 0.7302, 0.7292, 0.7296] \taverage = 0.7296\n",
      "\t\tValidation accuracies:  [0.7308, 0.729, 0.7272, 0.7314, 0.7287] \taverage = 0.7294\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7327, 0.7334, 0.7328, 0.7332, 0.7337] \taverage = 0.7332\n",
      "\t\tValidation accuracies:  [0.7353, 0.7316, 0.7346, 0.733, 0.7309] \taverage = 0.7331\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7337, 0.7333, 0.7335, 0.7337, 0.733] \taverage = 0.7334\n",
      "\t\tValidation accuracies:  [0.7322, 0.7346, 0.7323, 0.7328, 0.7349] \taverage = 0.7334\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.7341, 0.7338, 0.7335, 0.7333, 0.7341] \taverage = 0.7338\n",
      "\t\tValidation accuracies:  [0.7337, 0.7336, 0.7341, 0.7355, 0.7306] \taverage = 0.7335\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7302, 0.7294, 0.7287, 0.7297, 0.7295] \taverage = 0.7295\n",
      "\t\tValidation accuracies:  [0.7269, 0.7299, 0.732, 0.7291, 0.7293] \taverage = 0.7294\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7327, 0.7333, 0.7338, 0.7329, 0.7331] \taverage = 0.7332\n",
      "\t\tValidation accuracies:  [0.7346, 0.7328, 0.731, 0.735, 0.7321] \taverage = 0.7331\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7338, 0.7339, 0.7336, 0.734, 0.7331] \taverage = 0.7337\n",
      "\t\tValidation accuracies:  [0.7342, 0.7319, 0.7344, 0.7332, 0.734] \taverage = 0.7335\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.734, 0.7341, 0.7337, 0.7338, 0.7344] \taverage = 0.734\n",
      "\t\tValidation accuracies:  [0.7325, 0.7336, 0.7352, 0.734, 0.7336] \taverage = 0.7338\n",
      "Pre-processing hyperparameters: {'degree': 4, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7283, 0.7284, 0.7294, 0.7276, 0.728] \taverage = 0.7283\n",
      "\t\tValidation accuracies:  [0.7269, 0.7293, 0.7244, 0.7306, 0.7298] \taverage = 0.7282\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7326, 0.732, 0.7323, 0.7328, 0.733] \taverage = 0.7325\n",
      "\t\tValidation accuracies:  [0.731, 0.7348, 0.7324, 0.7332, 0.7306] \taverage = 0.7324\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7329, 0.7327, 0.7324, 0.7331, 0.733] \taverage = 0.7328\n",
      "\t\tValidation accuracies:  [0.7336, 0.7326, 0.7358, 0.7307, 0.7313] \taverage = 0.7328\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.7325, 0.7336, 0.7335, 0.7334, 0.7329] \taverage = 0.7332\n",
      "\t\tValidation accuracies:  [0.7359, 0.7331, 0.7316, 0.7317, 0.7331] \taverage = 0.7331\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7291, 0.729, 0.7292, 0.7301, 0.7308] \taverage = 0.7296\n",
      "\t\tValidation accuracies:  [0.7322, 0.7313, 0.73, 0.7288, 0.7254] \taverage = 0.7295\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7332, 0.7334, 0.7327, 0.7331, 0.7333] \taverage = 0.7331\n",
      "\t\tValidation accuracies:  [0.7324, 0.7321, 0.7348, 0.7336, 0.733] \taverage = 0.7332\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7334, 0.733, 0.7338, 0.734, 0.7332] \taverage = 0.7335\n",
      "\t\tValidation accuracies:  [0.7351, 0.7342, 0.7321, 0.7319, 0.7342] \taverage = 0.7335\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.7335, 0.7341, 0.7335, 0.7337, 0.734] \taverage = 0.7338\n",
      "\t\tValidation accuracies:  [0.7336, 0.733, 0.7323, 0.7352, 0.7339] \taverage = 0.7336\n",
      "Pre-processing hyperparameters: {'degree': 6, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.01}\n",
      "\t\tTraining accuracies:  [0.7294, 0.7291, 0.7297, 0.7297, 0.7298] \taverage = 0.7295\n",
      "\t\tValidation accuracies:  [0.7305, 0.7306, 0.7284, 0.7294, 0.7282] \taverage = 0.7294\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.05}\n",
      "\t\tTraining accuracies:  [0.7332, 0.7321, 0.7339, 0.7329, 0.7333] \taverage = 0.7331\n",
      "\t\tValidation accuracies:  [0.7329, 0.7357, 0.7293, 0.7357, 0.7316] \taverage = 0.733\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.1}\n",
      "\t\tTraining accuracies:  [0.7337, 0.7332, 0.7332, 0.7344, 0.7339] \taverage = 0.7337\n",
      "\t\tValidation accuracies:  [0.7323, 0.7348, 0.7372, 0.7321, 0.7309] \taverage = 0.7335\n",
      "\tModel hyperparameters: {'max_iters': 250, 'gamma': 0.5}\n",
      "\t\tTraining accuracies:  [0.7342, 0.7344, 0.7339, 0.7334, 0.7343] \taverage = 0.734\n",
      "\t\tValidation accuracies:  [0.7334, 0.7299, 0.7347, 0.7365, 0.7349] \taverage = 0.7339\n",
      "Optimal pre-processing parameters:  {'degree': 6, 'alpha': 1, 'beta': None}\n",
      "Optimal model parameters:  {'max_iters': 250, 'gamma': 0.5}\n",
      "Average K-fold validation accuracy: 0.7339\n",
      "Average K-fold training accuracy: 0.734\n",
      "Full training accuracy for optimal hyperparameters:  0.734\n"
     ]
    }
   ],
   "source": [
    "model_name = logistic_regression\n",
    "\n",
    "alphas = [1]\n",
    "betas = [None, 10]\n",
    "degrees = [4, 5, 6]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "gammas = [0.01, 0.05, 0.1, 0.5]\n",
    "maxs_iters = [250]\n",
    "hypsLR = [{'max_iters': mi, 'gamma': g} for g in gammas for mi in maxs_iters]\n",
    "\n",
    "optPreHypLR, optHypLR, optAccuracyLR = hyperparameterTuning(train_data, train_labels, K, model_name, preHyps, hypsLR, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameters, with corresponding validation accuracy and test accuracy**\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.5}    0.7329    0.7345   \n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.5}    0.7335    0.7345 \n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.5}    0.7338    0.7343\n",
    "\n",
    "- {'degree': 4, 'alpha': 1, 'beta': None} {'max_iters': 250, 'gamma': 0.5}    0.7331    0.7341   \n",
    "\n",
    "- {'degree': 5, 'alpha': 1, 'beta': 10} {'max_iters': 250, 'gamma': 0.5}    0.7336    0.7345\n",
    "\n",
    "- {'degree': 6, 'alpha': 1, 'beta': 10} {'max_iters': 250, 'gamma': 0.5}    0.7339   0.7345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7343\n"
     ]
    }
   ],
   "source": [
    "testAccLR = testAccuracy(logistic_regression, {'degree': 4, 'alpha': 1, 'beta': 10}, {'max_iters': 250, 'gamma': 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, we seem to reach an accuracy threshold at around 73%. There is low variance in the validation accuracy across all hyperparameter values. The $\\gamma = 0.5$ parameter value maximizes the validation accuracy across all degrees $d \\in \\{4,5,6\\}$. The $\\beta$ parameter leads to little to no change in the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularized logistic regression, we pick the optimal value of the hyperparameter $\\gamma = 0.1$ from logistic regression, then add additional tuning on the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7295, 0.7289, 0.7285, 0.7287, 0.7289] \taverage = 0.7289\n",
      "\t\tValidation accuracies:  [0.7263, 0.7288, 0.7302, 0.7294, 0.729] \taverage = 0.7287\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7291, 0.7292, 0.7297, 0.7286, 0.7276] \taverage = 0.7288\n",
      "\t\tValidation accuracies:  [0.7264, 0.7283, 0.7249, 0.73, 0.7337] \taverage = 0.7287\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.728, 0.728, 0.7273, 0.7272, 0.7275] \taverage = 0.7276\n",
      "\t\tValidation accuracies:  [0.7253, 0.7259, 0.729, 0.7279, 0.7296] \taverage = 0.7275\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7318, 0.7317, 0.7325, 0.7312, 0.7326] \taverage = 0.732\n",
      "\t\tValidation accuracies:  [0.7326, 0.7312, 0.7296, 0.7363, 0.7296] \taverage = 0.7319\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7322, 0.7322, 0.7321, 0.7315, 0.7315] \taverage = 0.7319\n",
      "\t\tValidation accuracies:  [0.7302, 0.7305, 0.7313, 0.7338, 0.7333] \taverage = 0.7318\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7322, 0.7303, 0.7313, 0.7321, 0.7308] \taverage = 0.7313\n",
      "\t\tValidation accuracies:  [0.73, 0.7352, 0.7322, 0.7284, 0.7314] \taverage = 0.7314\n",
      "Pre-processing hyperparameters: {'degree': 7, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7313, 0.7311, 0.7328, 0.7313, 0.7319] \taverage = 0.7317\n",
      "\t\tValidation accuracies:  [0.7323, 0.733, 0.7278, 0.7328, 0.7304] \taverage = 0.7313\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7325, 0.7309, 0.7316, 0.7309, 0.7321] \taverage = 0.7316\n",
      "\t\tValidation accuracies:  [0.7268, 0.735, 0.7318, 0.7336, 0.73] \taverage = 0.7314\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.732, 0.7315, 0.7306, 0.731, 0.731] \taverage = 0.7312\n",
      "\t\tValidation accuracies:  [0.7281, 0.7303, 0.7322, 0.7324, 0.7328] \taverage = 0.7312\n",
      "Pre-processing hyperparameters: {'degree': 9, 'alpha': 1, 'beta': None}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7317, 0.7308, 0.7307, 0.7313, 0.7305] \taverage = 0.731\n",
      "\t\tValidation accuracies:  [0.7277, 0.7326, 0.7318, 0.7299, 0.7328] \taverage = 0.731\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7317, 0.7312, 0.7303, 0.7307, 0.7307] \taverage = 0.7309\n",
      "\t\tValidation accuracies:  [0.7292, 0.729, 0.7334, 0.7315, 0.7316] \taverage = 0.7309\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7304, 0.7303, 0.731, 0.7303, 0.7299] \taverage = 0.7304\n",
      "\t\tValidation accuracies:  [0.7293, 0.7315, 0.7278, 0.7302, 0.7322] \taverage = 0.7302\n",
      "Pre-processing hyperparameters: {'degree': 3, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7291, 0.729, 0.7287, 0.7289, 0.7288] \taverage = 0.7289\n",
      "\t\tValidation accuracies:  [0.7266, 0.73, 0.73, 0.7289, 0.7286] \taverage = 0.7288\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7284, 0.7281, 0.7298, 0.7292, 0.7288] \taverage = 0.7289\n",
      "\t\tValidation accuracies:  [0.7308, 0.7323, 0.7242, 0.7285, 0.7287] \taverage = 0.7289\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7269, 0.7274, 0.728, 0.728, 0.7276] \taverage = 0.7276\n",
      "\t\tValidation accuracies:  [0.7297, 0.7272, 0.726, 0.7267, 0.7277] \taverage = 0.7275\n",
      "Pre-processing hyperparameters: {'degree': 5, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7315, 0.7318, 0.7316, 0.733, 0.732] \taverage = 0.732\n",
      "\t\tValidation accuracies:  [0.734, 0.7333, 0.7334, 0.7275, 0.7321] \taverage = 0.7321\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7324, 0.7319, 0.7327, 0.7318, 0.7308] \taverage = 0.7319\n",
      "\t\tValidation accuracies:  [0.7293, 0.7324, 0.7297, 0.7324, 0.7355] \taverage = 0.7319\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7317, 0.7311, 0.7314, 0.7314, 0.7311] \taverage = 0.7313\n",
      "\t\tValidation accuracies:  [0.729, 0.7341, 0.7328, 0.73, 0.7308] \taverage = 0.7313\n",
      "Pre-processing hyperparameters: {'degree': 7, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7322, 0.7323, 0.7315, 0.7302, 0.7319] \taverage = 0.7316\n",
      "\t\tValidation accuracies:  [0.729, 0.7294, 0.7326, 0.7361, 0.7304] \taverage = 0.7315\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7322, 0.7315, 0.7309, 0.7317, 0.7316] \taverage = 0.7316\n",
      "\t\tValidation accuracies:  [0.729, 0.7324, 0.7353, 0.7304, 0.7311] \taverage = 0.7316\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7312, 0.7305, 0.7321, 0.7306, 0.7318] \taverage = 0.7312\n",
      "\t\tValidation accuracies:  [0.731, 0.7345, 0.728, 0.7324, 0.7296] \taverage = 0.7311\n",
      "Pre-processing hyperparameters: {'degree': 9, 'alpha': 1, 'beta': 10}\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "\t\tTraining accuracies:  [0.7319, 0.7319, 0.7306, 0.7301, 0.7308] \taverage = 0.7311\n",
      "\t\tValidation accuracies:  [0.7273, 0.7275, 0.7338, 0.7341, 0.7323] \taverage = 0.731\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.01}\n",
      "\t\tTraining accuracies:  [0.7312, 0.7308, 0.7313, 0.7307, 0.7308] \taverage = 0.731\n",
      "\t\tValidation accuracies:  [0.7297, 0.7318, 0.7303, 0.7313, 0.731] \taverage = 0.7308\n",
      "\tModel hyperparameters: {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.1}\n",
      "\t\tTraining accuracies:  [0.7307, 0.73, 0.7303, 0.7302, 0.7307] \taverage = 0.7304\n",
      "\t\tValidation accuracies:  [0.7287, 0.732, 0.729, 0.732, 0.7293] \taverage = 0.7302\n",
      "Optimal pre-processing parameters:  {'degree': 5, 'alpha': 1, 'beta': None}\n",
      "Optimal model parameters:  {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}\n",
      "Average K-fold validation accuracy: 0.7321\n",
      "Average K-fold training accuracy: 0.732\n",
      "Full training accuracy for optimal hyperparameters:  0.7318\n"
     ]
    }
   ],
   "source": [
    "model_name = reg_logistic_regression\n",
    "\n",
    "alphas = [1]\n",
    "betas = [None, 10]\n",
    "degrees = [3, 5, 7, 9]\n",
    "preHyps = [{'degree' : d, 'alpha' : a, 'beta': b} for a in alphas for b in betas for d in degrees]\n",
    "\n",
    "gammas = [0.01]\n",
    "maxs_iters = [500]\n",
    "lambdas = [0.001, 0.01, 0.1]\n",
    "hypsRLR = [{'max_iters': mi, 'gamma': g, 'lambda_' : l} \n",
    "                      for g in gammas for mi in maxs_iters for l in lambdas]\n",
    "\n",
    "optPreHypRLR, optHypRLR, optAccuracyRLR = hyperparameterTuning(train_data, train_labels, K, model_name, preHyps, hypsRLR, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for logistic regression, we see that the hyperparameters have little effect on the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7319\n"
     ]
    }
   ],
   "source": [
    "testAccRLR = testAccuracy(reg_logistic_regression, {'degree': 5, 'alpha': 1, 'beta': None}, {'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test accuracy of each model**\n",
    "\n",
    "Now that we have the optimal hyperparameters for each model, we can predict the test labels and see which model yields the highest test accuracy: \n",
    "\n",
    "- LS {'degree': 5, 'alpha': 1, 'beta': None}: train: 0.7949, valid: 0.7843, test: 0.664 \n",
    "- LS {'degree': 5, 'alpha': 1, 'beta': 5}: train: 0.7949, valid: 0.6587, test: 0.707    \n",
    "- LS {'degree': 5, 'alpha': 1, 'beta': 10}: train: 0.7949, valid: 0.7634, test:0.707    MAX\n",
    "- RR {'degree': 6, 'alpha': 1, 'beta': None}{'lambda_': 1e-06}: train: 0.7976, valid: 0.7974 test: 0.786 MAX\n",
    "- RR {'degree': 7, 'alpha': 1, 'beta': None}{'lambda_': 1e-05} train: 0.7965, valid: 0.7957, test: 0.784\n",
    "- GD {'degree': 4, 'alpha': 1, 'beta': 5}{'max_iters': 500, 'gamma': 0.1}, train: 0.7772, valid: 0.7725, test: 0.35\n",
    "- GD {'degree': 4, 'alpha': 1, 'beta': None}{'max_iters': 500, 'gamma': 0.1}, train: 0.772, valid: 0.7724, test: 0.776 MAX\n",
    "- SGD {'degree': 5, 'alpha': 1, 'beta': 10}{'max_iters': 500, 'gamma': 0.01, 'batch_size': 100}, train: 0.7491, valid:0.7482, test: 0.753\n",
    "- LR {'degree': 4, 'alpha': 1, 'beta': None}{'max_iters': 250, 'gamma': 0.1} train: 0.7329, valid: 0.7327, test: 0.734\n",
    "- RLR {'degree': 5, 'alpha': 1, 'beta': None}{'max_iters': 500, 'gamma': 0.01, 'lambda_': 0.001}, train: 0.7318, valid: 0.7319, test: 0.733\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "accuracy_types = ['Training', 'Validation', 'Test']\n",
    "model_names = ['LS', 'RR', 'GD', 'SGD', 'LR', 'RLR']\n",
    "trainAccs = [0.7949, 0.7976, 0.7772, 0.7491, 0.7329, 0.7318]\n",
    "valAccs = [0.7634, 0.7974, 0.7724, 0.7482, 0.7327, 0.7319]\n",
    "testAccs = [0.707, 0.786, 0.776, 0.753, 0.734, 0.733]\n",
    "df = pd.DataFrame(columns=['Model', 'Accuracy type', 'Accuracy'])\n",
    "\n",
    "for i in range(6):\n",
    "    df.loc[len(df), df.columns] = model_names[i], 'Training', trainAccs[i]\n",
    "    df.loc[len(df), df.columns] = model_names[i], 'Validation', valAccs[i]\n",
    "    df.loc[len(df), df.columns] = model_names[i], 'Test', testAccs[i]\n",
    "        \n",
    "sns.set_theme()\n",
    "g = sns.barplot(x=\"Model\", y='Accuracy', hue=\"Accuracy type\",data=df, palette=\"blend:#2AB,#7D7\")\n",
    "g.set(ylim=(0.68, 0.82))#, title='Optimal 5-fold cross-validation training and validation accuracies \\n and resulting test accuracy of several models on the Higgs Boson dataset')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
